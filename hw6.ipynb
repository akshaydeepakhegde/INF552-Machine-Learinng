{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import scipy  \n",
    "import scikits.bootstrap as bootstrap\n",
    "import statsmodels.stats.api as sms\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from ggplot import *\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Imputer\n",
    "from scipy.stats import variation\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn import cross_validation\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.datasets import make_classification\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import hamming_loss,accuracy_score\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "\n",
    "#read the files\n",
    "with io.open(path, encoding='utf-8') as f:\n",
    "    text = f.read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "\n",
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=60,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import util\n",
    "\n",
    "# Create the neural network\n",
    "def conv_net(x_dict, height, width, reuse):\n",
    "    # Define a scope for reusing the variables\n",
    "    with tf.variable_scope('ConvNet', reuse=reuse):\n",
    "        # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
    "        x = x_dict['images']\n",
    "        x = tf.reshape(x, shape=[-1, height, width, 1])\n",
    "        # Convolution Layer 1 with 16 filters and a kernel size of 4 repeated twice\n",
    "        conv1_1 = tf.layers.conv2d(x, 16, 5, activation=tf.nn.relu)\n",
    "        conv1_p = tf.layers.max_pooling2d(conv1_1, 2, 2)\n",
    "        conv1_2 = tf.layers.conv2d(conv1_p, 16, 5, activation=tf.nn.relu)\n",
    "        conv1_3 = tf.layers.batch_normalization(conv1_2, axis=-1, momentum=0.99, epsilon = 0.001,\n",
    "        center=True, scale=True, training=True)\n",
    "        conv1_4 = tf.layers.max_pooling2d(conv1_3, 2, 2)\n",
    "        # Convolution Layer 2 with 32 filters and a kernel size of 4\n",
    "        conv2_1 = tf.layers.conv2d(conv1_3, 32, 3, activation=tf.nn.relu)\n",
    "        conv2_p = tf.layers.max_pooling2d(conv2_1, 2, 2)\n",
    "        conv2_2 = tf.layers.conv2d(conv2_p, 32, 3, activation=tf.nn.relu)\n",
    "        conv2_3 = tf.layers.batch_normalization(conv2_2, axis=-1, momentum=0.99, epsilon = 0.001,\n",
    "        center=True, scale=True, training=True)\n",
    "        conv2_4 = tf.layers.max_pooling2d(conv2_3, 2, 2)\n",
    "        # Convolution Layer 3 with 64 filters and a kernel size of 2\n",
    "        conv3_1 = tf.layers.conv2d(conv2_3, 64, 2, activation=tf.nn.relu)\n",
    "        conv3_p = tf.layers.max_pooling2d(conv3_1, 2, 2)\n",
    "        conv3_2 = tf.layers.conv2d(conv3_1, 64, 2, activation=tf.nn.relu)                \n",
    "        conv3_3 = tf.layers.batch_normalization(conv3_2, axis=-1, momentum=0.99, epsilon = 0.001,\n",
    "        center=True, scale=True, training=True)        \n",
    "        conv3_4 = tf.layers.max_pooling2d(conv3_3, 2, 2)\n",
    "        # Output layer, a, b distribution \n",
    "        out = tf.layers.conv2d(conv3_3, 128, 1, activation=tf.nn.softmax)\n",
    "        out = tf.contrib.layers.flatten(out)\n",
    "    return out\n",
    "        \n",
    "def model_fn(features, labels, mode):\n",
    "    #Build the CNN\n",
    "    logits_train = conv_net(features,64,64,reuse=False)\n",
    "    #If in prediction mode end early\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    return tf.estimator.EstimatorSpec(mode, logits_train)\n",
    "\n",
    "    #Define Loss and Optimizer\n",
    "    labels = tf.contrib.layers.flatten(labels)\n",
    "    loss_op = tf.contrib.losses.mean_squared_error(logits_train, labels)\t\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op)\n",
    "    \n",
    "    # Evaluate the accuracy of the model \n",
    "    acc_op = tf.metrics.accuracy(labels,logits_train)\n",
    "\n",
    "    estim_specs = tf.estimator.EstimatorSpec(mode=mode,loss=loss_op, train_op=train_op, eval_metric_ops={'accuracy': acc_op})\n",
    "\n",
    "    return estim_specs\n",
    "#--------------------------- Train Vars --------------------------------------------------------\n",
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = 2000\n",
    "batch_size = 2500 #?\n",
    "img_size = 64\n",
    "data_path = '/home/steven/CMPS/Images/pj_prepro/64x64'\n",
    "X_train, Y_train_rgb, mean_image = util.load_databatch(data_path, 1, img_size)\n",
    "num_input = X_train.shape[0]\n",
    "for j in range(num_input):\n",
    "        X_train[j] = util.preprocess_image(X_train[j])\n",
    "L_train = X_train[:num_input-batch_size,:,:,0] #L-channel only\n",
    "ab_train = X_train[:num_input-batch_size,:,:,1:] #ab-channels\n",
    "L_test = X_train[num_input-batch_size:,:,:,0] #L-channel only\n",
    "ab_test = X_train[num_input-batch_size:,:,:,1:] #ab-channels\n",
    "\n",
    "#Build the estimator\n",
    "model = tf.estimator.Estimator(model_fn)\n",
    "\n",
    "#Define the input function for training \n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(x={'images':L_train}, y=ab_train,\n",
    "batch_size=batch_size, num_epochs=None, shuffle=True)\n",
    "\n",
    "# Train the Model\n",
    "model.train(input_fn, steps=num_steps)\n",
    "\n",
    "\n",
    "#Evaluate Model\n",
    "#Input test  \n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(x={'images':L_test}, y=ab_test,\n",
    "batch_size=batch_size, num_epochs=None, shuffle=False)\n",
    "e = model.evaluate(input_fn)\n",
    "\n",
    "print(\"Testing Accuracy:\", e['accuracy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
